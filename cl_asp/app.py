import re, string, random, itertools, logging, json
from collections import Counter

import numpy as np
import pandas as pd
from flask import Flask, render_template, request
from gensim.models.doc2vec import Doc2Vec
from gensim.similarities.index import AnnoyIndexer
from umap import UMAP
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN, OPTICS
from sklearn.preprocessing import StandardScaler, MinMaxScaler

word_df = pd.read_csv("./data/app/word_df_100k.csv")
sentence_df = pd.read_csv("./data/app/sentence_df_100k.csv")
sentence_df.dropna(inplace=True)

# kv = KeyedVectors.load("./models/enron_100k_vectors.kv", mmap='r')

app = Flask(__name__)
projection_options = {1: PCA(n_components=2), 2: UMAP(n_components=2), 3: TSNE(n_components=2)}
cluster_options = {1: None, 2: DBSCAN(eps=.6, min_samples=3), 3: OPTICS()}
iob_map = None

class Session():
    '''Session class manages the state of the interactive elements in the session'''
    def __init__(self):
        # Pretrained model
        self.model = Doc2Vec.load('./models/enron_100k_d2v_model')
        # All words generated by request
        self.words = []
        # Vector repres of those words
        self.vectors = []
        # Projection options
        self.projection = None
        # Cluster options
        self.cluster = None
        # Words currently selected on the projection
        self.selected_words = []
        # Dict of task monitors, accessable with class label the monitor is associated with
        self.task_queue = {}

    def set_words(self, words):
        self.words = words

    def set_vectors(self, vectors):
        self.vectors = vectors

    def set_projection(self, function):
        self.projection = function

    def set_cluster(self, function):
        self.cluster = function

    def set_selected(self, words):
        if isinstance(words, list):
            self.selected_words = words
        else:
            self.selected_words.append(words)

    def create_task(self, label):
        self.task_queue.update({label : Monitor(label)})

    def task_progress(self, label):
        return self.task_queue[label].progress()

class Monitor():
    '''Monitor class manages the ongoing annotation tasks'''
    def __init__(self, label):
        # The class label assigned in this task
        self.label = label
        # Words associated with the task
        self.words = set()
        # Set of sentence indices which contain words in the word set
        self.sentence_queue = set()
        # Set of indices manually annotated as being examples of the class
        self.accepted = set()
        # Set of indices manually annotated as being not examples of the class
        self.rejected = set()

    def update_words(self, entry):
        self.words.update(entry)

    def find_sentences(self):
        words = self.words
        out=set()
        if not words:
            print("no words selected")
        else:
            for index, sentence in enumerate(sentence_df.text.values):
                if len(words.intersection(set(sentence.split()))) > 0:
                    out.add(index)
        return out

    def update_sentence_queue(self, entry):
        self.sentence_queue.update(entry)

    def progress(self):
        # For client-side task progression metrics
        return [len(self.words), len(self.sentence_queue), len(self.accepted), len(self.rejected)]

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/get_progress")
def get_progress():
    label = request.args.get('label').upper()
    return json.dumps({'response': session.task_progress(label)})

@app.route("/set_projection")
def set_projection():
    choice = int(request.args.get('value'))
    session.set_projection(projection_options[choice])
    return json.dumps('{}: projection set'.format(choice))

@app.route("/set_cluster")
def set_cluster():
    choice = int(request.args.get('value'))
    session.set_cluster(cluster_options[choice])
    return json.dumps('{}: cluster set'.format(choice))

@app.route("/set_vectors")
def set_vectors():
    word = request.args.get('word').lower()
    n = int(request.args.get('size'))
    try:
        result = np.asarray(session.model.wv.most_similar(word, topn=n))[:,0]
        result = np.append(result, word)
        session.set_words(result)
    except KeyError as e:
        return json.dumps(False)

    vecs = np.asarray([(session.model.wv[key]) for key in result])
    # <?> Maybe offload this section into another function, else set set_vectors
    # needs to be called for every update of proj/clust options.
    emb = session.projection.fit_transform(vecs)
    session.set_vectors(emb)
    return json.dumps('{}: vectors set'.format(word))

@app.route("/get_scatter_data")
def get_scatter_data():
    update = int(request.args.get('option'))
    if session.cluster is None:
        iob_list = [word_df[word_df['word']==word].iob.values[0] for word in session.words]
        iob_map = {t: i for i, t in enumerate(set(iob_list))}
        labels = [iob_map[i] for i in iob_list]
    else:
        # <!> JSON might be funny about label dtypes.
        clust = session.cluster.fit(session.vectors)
        labels = list(map(int, clust.labels_))
    try:
        assert len(session.vectors) == len(session.words) == len(labels)
    except AssertionError:
        print('Data error: unequal number of elements')
        return json.dumps({'response': False})

    selected = list(map(bool, np.zeros(len(session.vectors))))
    session.set_selected([])

    if not update:
        selected[-1] = True
        session.set_selected(session.words[-1])
    response = [{"index": i, "x": x.astype(float),
                 "y": y.astype(float),
                 "word": word,
                 "label": label,
                 "selected": select} for i, ((x, y), word, label, select) in enumerate(zip(session.vectors, session.words, labels, selected))]
    return json.dumps({'response': response})

@app.route("/set_selected")
def set_selected():
    # Update session context WRT the word vectors selected
    mask = request.args.get('mask').strip('[]').split(',')
    mask = [int(i) for i in mask]
    words = [j for i, j in zip(mask, session.words) if i]
    session.set_selected(words)
    return json.dumps({'response': True})

@app.route("/get_selected")
def get_selected():
    return json.dumps({'response': session.selected_words})

@app.route("/set_word_labels")
def set_word_labels():
    # Update IOB tags within word_df to reflect annotations
    label = request.args.get('label').upper()
    indices = [word_df[word_df['word']==w].index.values[0] for w in session.selected_words]
    for i in indices:
        word_df.at[i, 'iob'] = label
    return json.dumps({'response': True})

@app.route("/get_monitor_words")
def get_monitor_words():
    label = request.args.get('monitor_id').upper()
    return json.dumps({'response': list(session.task_queue[label].words)})

@app.route("/update_queue")
def update_queue():
    label = request.args.get('label').upper()
    # If monitor doesn't exist, make it
    if label not in session.task_queue:
        session.create_task(label)
    else:
        # Access monitor with class label ...
        monitor = session.task_queue[label]
        # update associated words ...
        monitor.update_words(session.selected_words)
        # and find sentences containing those words:
        monitor.update_sentence_queue(monitor.find_sentences())
    return json.dumps({'response': True})

@app.route("/get_queue_length")
def get_queue_length():
    monitor = session.task_queue[request.args.get('monitor_id').upper()]
    return json.dumps(({'response': len(monitor.sentence_queue)}))

@app.route("/get_batch_sentences")
def get_batch_sentences():
    monitor = session.task_queue[request.args.get('monitor_id').upper()]
    batch_size = int(request.args.get('batch_size'))
    s = []
    if len(monitor.sentence_queue) > batch_size:
        batch = random.sample(monitor.sentence_queue, batch_size)
    else:
        batch = random.sample(monitor.sentence_queue, len(monitor.sentence_queue))
    return json.dumps({'sentence': list(sentence_df.iloc[batch].text), 'indices': batch})

@app.route("/update_accept")
def update_accept():
    monitor = session.task_queue[request.args.get('monitor_id').upper()]
    index = int(request.args.get('index'))
    monitor.sentence_queue.remove(index)
    monitor.accepted.update([index])
    return json.dumps({'response': True})

@app.route("/update_reject")
def update_reject():
    monitor = session.task_queue[request.args.get('monitor_id').upper()]
    index = int(request.args.get('index'))
    monitor.sentence_queue.remove(index)
    monitor.rejected.update([index])
    return json.dumps({'response': True})

@app.route("/update_skip")
def update_skip():
    return json.dumps({'response': True})

@app.route("/export_data")
def export_data():
    # Make reference out of word_df entries:
    word_dict = dict(zip(word_df.word, word_df.iob))
    default_value = 'O'
    df_list = []
    for task in session.task_queue.values():
        subset = sentence_df.iloc[list(task.accepted)]
        if len(subset):
            out = []
            for text in subset.text:
                iob_tags = []
                beginning = True
                for word in text.split():
                    try:
                        tag = word_dict[word]
                        if tag != 'O' and beginning:
                            iob_tags.append("B-" + tag)
                            beginning = False
                        elif tag != 'O' and not beginning:
                            iob_tags.append("I-" + tag)
                        else:
                            iob_tags.append(word_dict[word])
                            beginning = True
                    except KeyError as e:
                        # <?> word_df is made from d2v_model.wv.vocab.keys().
                        # Therefore, when d2v_model is made with min_count= (n > 1),
                        # infrequent words get dropped, and so some words found within
                        # sentence_df are not modelled.
                        iob_tags.append(default_value)
                        beginning = True
                out.append(', '.join(iob_tags))
            subset['iob'] = out
            subset['label'] = task.label
            df_list.append(subset)
        else:
            print(task.label, "currently unannotated")
    if df_list:
        result = pd.concat(df_list)
        result.to_csv('./out/test.csv', index=False)
    else:
        return json.dumps({'response': False})
    return json.dumps({'response': True})

if __name__ == '__main__':
    session = Session()
    app.run(port=8080, debug=True)
