{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and Modelling of Enron Dataset\n",
    "<br>\n",
    "The following outlines the methods used to pre-process the Enron dataset and create the Doc2Vec model that forms the basis of the annotation application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# To get console output when training Doc2vec:\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Enron Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "master_df = pd.read_csv('./data/enron/emails.csv')\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-processing\n",
    "#### 2.a Separate the desired features within the text of the raw message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_message(raw_message):\n",
    "    lines = raw_message.split('\\n')\n",
    "    email = {}\n",
    "    # These keys appear in the body of the messages.\n",
    "    # Not all messages contain the same keys nor in equal quantity.\n",
    "    # For example, in forwarded messages, 'to' and 'from' appear more than once, and so can become overwritten.\n",
    "    keys_to_extract = ['message-id']\n",
    "    hits = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == '': # this approach cannot separate forwarded messages.\n",
    "            email['body'] = ' '.join([l for l in lines[i+1:]]).strip()\n",
    "            break\n",
    "        elif (hits < len(keys_to_extract)):\n",
    "            try:\n",
    "                pairs = line.split(':')\n",
    "                key = pairs[0].lower()\n",
    "                val = pairs[1].lower().strip()\n",
    "                if (key in keys_to_extract):\n",
    "                    email[key] = val.strip()\n",
    "                    hits += 1\n",
    "            except IndexError as e:\n",
    "                hits += 1\n",
    "                pass\n",
    "    return email\n",
    "\n",
    "def map_to_list(emails, key):\n",
    "    results = []\n",
    "    for email in emails:\n",
    "        if key not in email:\n",
    "            results.append('')\n",
    "        else:\n",
    "            results.append(email[key])\n",
    "    return results\n",
    "\n",
    "def parse_into_emails(messages):\n",
    "    emails = [parse_raw_message(message) for message in messages]\n",
    "    return {\n",
    "        'id': map_to_list(emails, 'message-id'),\n",
    "        'body': map_to_list(emails, 'body'),\n",
    "    }\n",
    "\n",
    "def test_nulls_and_empties(df):\n",
    "    print(\"Null values:\", df.isnull().values.any()) # Doesn't find empty strings.\n",
    "    x = [len(df[df[header]==''].values) for header in list(df)]\n",
    "    if sum(x):\n",
    "        print(\"Empty strings:\")\n",
    "        for a in zip(list(df), x):      \n",
    "            print(a[0], \":\", a[1])\n",
    "    else:\n",
    "        print(\"Empty strings: False\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values: False\n",
      "Empty strings: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;12437885.1075852968487.javamail.evans@thyme&gt;</td>\n",
       "      <td>finally read it - I thought it was pretty crap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;6631704.1075851015333.javamail.evans@thyme&gt;</td>\n",
       "      <td>What about the questions I asked about whether...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;22940715.1075840010060.javamail.evans@thyme&gt;</td>\n",
       "      <td>I've killed the Cal-Imbalance deal I entered u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;5148853.1075841066243.javamail.evans@thyme&gt;</td>\n",
       "      <td>There have been many occasions where individua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;1122724.1075846012741.javamail.evans@thyme&gt;</td>\n",
       "      <td>---------------------- Forwarded by Kay Mann/C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              id  \\\n",
       "0  <12437885.1075852968487.javamail.evans@thyme>   \n",
       "1   <6631704.1075851015333.javamail.evans@thyme>   \n",
       "2  <22940715.1075840010060.javamail.evans@thyme>   \n",
       "3   <5148853.1075841066243.javamail.evans@thyme>   \n",
       "4   <1122724.1075846012741.javamail.evans@thyme>   \n",
       "\n",
       "                                                body  \n",
       "0  finally read it - I thought it was pretty crap...  \n",
       "1  What about the questions I asked about whether...  \n",
       "2  I've killed the Cal-Imbalance deal I entered u...  \n",
       "3  There have been many occasions where individua...  \n",
       "4  ---------------------- Forwarded by Kay Mann/C...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have opted to demonstrate on a very small subsample to speed up both the subsequent processes.\n",
    "# For reference, the master contains 517,401 entries, so feel free to change it for realistic results.\n",
    "n = 1000\n",
    "email_df = pd.DataFrame(parse_into_emails(master_df.sample(n).message))\n",
    "test_nulls_and_empties(email_df)\n",
    "email_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b Tokenise into sentences and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    # <3 ~ https://regex101.com/ \n",
    "    # Remove contiguous repreats of the same character of punctuation:\n",
    "    s = re.sub(r\"(([^\\w\\s])\\2+)\", r\"\\2\", s).lower()\n",
    "    # Remove some bizarre formatting artifacts specific to this dataset:\n",
    "    s = re.sub(r'(= |=0f\\\\|=0f|0f|=\\d+)', \"\", s)\n",
    "    # Remove any punctuation that's attached to whitespace (L/R) or occurs at the begining or end of the text:\n",
    "    s = re.sub(r\"((?<=\\s)[^\\w\\s])|([^\\w\\s](?=$|\\s))\", \"\", s).lstrip(string.punctuation)\n",
    "    # Again to catch e.g. \"(UWE),\" -> \"UWE)\" -> \"UWE\" - need proper solution:\n",
    "    s = re.sub(r\"((?<=\\s)[^\\w\\s])|([^\\w\\s](?=$|\\s))\", \"\", s).lstrip(string.punctuation)\n",
    "    return re.sub(r' +', ' ', s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Each document is tokenised into sentences, then those sentences are given an identifier and are cleaned.\n",
    "doc_id, sen_id, text = [], [], []\n",
    "for i, doc in enumerate(email_df.body):\n",
    "    sent_tokens = sent_tokenize(doc)\n",
    "    for j, sent in enumerate(sent_tokens):\n",
    "        doc_id.append(i)\n",
    "        sen_id.append(j)\n",
    "        text.append(clean_text(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sen_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>finally read it i thought it was pretty crappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i am spending all my days in credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>head above water but only just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>what about the questions i asked about whether...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000 especially if the state has the verticall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  sen_id                                               text\n",
       "0       0       0     finally read it i thought it was pretty crappy\n",
       "1       0       1                i am spending all my days in credit\n",
       "2       0       2                     head above water but only just\n",
       "3       1       0  what about the questions i asked about whether...\n",
       "4       1       1  2000 especially if the state has the verticall..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df = pd.DataFrame({'doc_id': doc_id, 'sen_id': sen_id, 'text':text})\n",
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-11 13:06:37,981 : INFO : collecting all words and their counts\n",
      "2020-04-11 13:06:37,982 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-04-11 13:06:38,034 : INFO : PROGRESS: at example #10000, processed 192427 words (3733112/s), 22707 word types, 10000 tags\n",
      "2020-04-11 13:06:38,038 : INFO : collected 23815 word types and 10699 unique tags from a corpus of 10699 examples and 205162 words\n",
      "2020-04-11 13:06:38,040 : INFO : Loading a fresh vocabulary\n",
      "2020-04-11 13:06:38,061 : INFO : effective_min_count=2 retains 10776 unique words (45% of original 23815, drops 13039)\n",
      "2020-04-11 13:06:38,062 : INFO : effective_min_count=2 leaves 192123 word corpus (93% of original 205162, drops 13039)\n",
      "2020-04-11 13:06:38,108 : INFO : deleting the raw counts dictionary of 23815 items\n",
      "2020-04-11 13:06:38,112 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2020-04-11 13:06:38,113 : INFO : downsampling leaves estimated 157246 word corpus (81.8% of prior 192123)\n",
      "2020-04-11 13:06:38,151 : INFO : estimated required memory for 10776 words and 100 dimensions: 18288400 bytes\n",
      "2020-04-11 13:06:38,152 : INFO : resetting layer weights\n",
      "2020-04-11 13:06:38,472 : INFO : training model with 4 workers on 10776 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2020-04-11 13:06:39,385 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:39,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:39,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:39,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:39,404 : INFO : EPOCH - 1 : training on 205162 raw words (167938 effective words) took 0.9s, 182925 effective words/s\n",
      "2020-04-11 13:06:40,186 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:40,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:40,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:40,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:40,204 : INFO : EPOCH - 2 : training on 205162 raw words (167840 effective words) took 0.8s, 214675 effective words/s\n",
      "2020-04-11 13:06:41,399 : INFO : EPOCH 3 - PROGRESS: at 81.99% examples, 118304 words/s, in_qsize 4, out_qsize 0\n",
      "2020-04-11 13:06:42,240 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:42,350 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:42,429 : INFO : EPOCH 3 - PROGRESS: at 97.32% examples, 74145 words/s, in_qsize 1, out_qsize 1\n",
      "2020-04-11 13:06:42,455 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:42,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:42,501 : INFO : EPOCH - 3 : training on 205162 raw words (167857 effective words) took 2.3s, 73829 effective words/s\n",
      "2020-04-11 13:06:43,319 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:43,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:43,332 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:43,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:43,340 : INFO : EPOCH - 4 : training on 205162 raw words (168052 effective words) took 0.8s, 210969 effective words/s\n",
      "2020-04-11 13:06:44,145 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:44,149 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:44,150 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:44,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:44,157 : INFO : EPOCH - 5 : training on 205162 raw words (167782 effective words) took 0.8s, 210985 effective words/s\n",
      "2020-04-11 13:06:44,965 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:44,967 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:44,973 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:44,982 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:44,983 : INFO : EPOCH - 6 : training on 205162 raw words (168045 effective words) took 0.8s, 206147 effective words/s\n",
      "2020-04-11 13:06:45,850 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:45,853 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:45,864 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:45,868 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:45,871 : INFO : EPOCH - 7 : training on 205162 raw words (168173 effective words) took 0.9s, 192229 effective words/s\n",
      "2020-04-11 13:06:46,647 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:46,655 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:46,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:46,672 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:46,673 : INFO : EPOCH - 8 : training on 205162 raw words (168133 effective words) took 0.8s, 214359 effective words/s\n",
      "2020-04-11 13:06:48,105 : INFO : EPOCH 9 - PROGRESS: at 66.18% examples, 80058 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-11 13:06:48,855 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:48,855 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:48,865 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:48,865 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:48,875 : INFO : EPOCH - 9 : training on 205162 raw words (168070 effective words) took 2.2s, 76671 effective words/s\n",
      "2020-04-11 13:06:49,584 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:49,594 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:49,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:49,615 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:49,615 : INFO : EPOCH - 10 : training on 205162 raw words (168043 effective words) took 0.7s, 235831 effective words/s\n",
      "2020-04-11 13:06:50,937 : INFO : EPOCH 11 - PROGRESS: at 81.99% examples, 105787 words/s, in_qsize 4, out_qsize 0\n",
      "2020-04-11 13:06:51,752 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:51,778 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:51,792 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:51,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:51,850 : INFO : EPOCH - 11 : training on 205162 raw words (168032 effective words) took 2.2s, 75513 effective words/s\n",
      "2020-04-11 13:06:52,688 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:52,689 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:52,694 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:52,699 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:52,700 : INFO : EPOCH - 12 : training on 205162 raw words (168017 effective words) took 0.8s, 203040 effective words/s\n",
      "2020-04-11 13:06:54,324 : INFO : EPOCH 13 - PROGRESS: at 87.45% examples, 90970 words/s, in_qsize 3, out_qsize 1\n",
      "2020-04-11 13:06:54,333 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:54,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:54,407 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:54,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-11 13:06:54,493 : INFO : EPOCH - 13 : training on 205162 raw words (168054 effective words) took 1.8s, 94204 effective words/s\n",
      "2020-04-11 13:06:55,565 : INFO : EPOCH 14 - PROGRESS: at 66.18% examples, 112576 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-11 13:06:55,714 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:55,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:55,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:55,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:55,735 : INFO : EPOCH - 14 : training on 205162 raw words (168184 effective words) took 1.2s, 141429 effective words/s\n",
      "2020-04-11 13:06:56,494 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:56,494 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:56,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:56,514 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:56,514 : INFO : EPOCH - 15 : training on 205162 raw words (167940 effective words) took 0.8s, 219755 effective words/s\n",
      "2020-04-11 13:06:57,264 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-11 13:06:57,274 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-11 13:06:57,279 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-11 13:06:57,284 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-11 13:06:57,289 : INFO : EPOCH - 16 : training on 205162 raw words (168085 effective words) took 0.8s, 221332 effective words/s\n",
      "2020-04-11 13:06:57,289 : INFO : training on a 3282592 raw words (2688245 effective words) took 18.8s, 142859 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gensim requires that data be converted into TaggedDocument format prior to training\n",
    "# The tags associated with each sentence is simply an integer to denoting the order in which it is stored in the dataframe.\n",
    "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(sentence_df.text.values)]\n",
    "d2v_model = Doc2Vec(documents, \n",
    "                    vector_size=100,\n",
    "                    alpha=0.025, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=2,\n",
    "#                     sample=(0, 1e-5),\n",
    "                    dm=1, \n",
    "                    window=4,\n",
    "                    workers=4,\n",
    "                    seed=1,\n",
    "                    epochs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save/Load\n",
    "\n",
    "#### 4.a  Doc2Vec/KeyedVectors\n",
    "<i>Uncomment blocks as required</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # <!> SAVE:\n",
    "# # Saves to C:\\Users\\<username>\\AppData\\Local\\Temp\n",
    "# from gensim.test.utils import get_tmpfile\n",
    "# fname = get_tmpfile(\"enron_100k_d2v_model\")\n",
    "# d2v_model.save(fname)\n",
    "\n",
    "# # Get word vectors:\n",
    "# word_vectors = d2v_model.wv\n",
    "# fname = get_tmpfile(\"enron_100k_vectors.kv\")\n",
    "# word_vectors.save(fname)\n",
    "\n",
    "# # # <!> LOAD:\n",
    "# # from gensim.models import KeyedVectors\n",
    "# # fname = './models/enron/enron_100k_d2v_model'\n",
    "# # model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n",
    "\n",
    "# # fname = './models/enron/enron_100k_vectors.kv'\n",
    "# # word_vectors = KeyedVectors.load(fname, mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b Save DataFrames used within the application\n",
    "<i>Uncomment as required</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the default IOB tag to the dataframe containing all vocabulary\n",
    "# word_df = pd.DataFrame({'word':list(d2v_model.wv.vocab.keys()), 'iob' : 'O'})\n",
    "\n",
    "# prefix = \"./data/app/\"\n",
    "# email_df.to_csv(prefix + 'email_df_<n>.csv', index=False)\n",
    "# sentence_df.to_csv(prefix + 'sentence_df_<n>.csv', index=False)\n",
    "# word_df.to_csv(prefix + 'word_df_<n>.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
